{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pickle\n",
    "import requests\n",
    "import math\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import cfscrape\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('/Users/macbook/Downloads/Chrome Driver/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting products links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_collection = ['https://www.obi.de/gartenbau/terrassendielen/c/1464',\n",
    "                       'https://www.obi.de/laminat-parkett-vinylboeden/laminat/c/514',\n",
    "                       'https://www.obi.de/laminat-parkett-vinylboeden/musterboeden/c/2193',\n",
    "                       'https://www.obi.de/laminat-parkett-vinylboeden/korkboeden/c/1174',\n",
    "                       'https://www.obi.de/bauen/bauholz/c/169',\n",
    "                       'https://www.obi.de/gartenbau/holzfliesen/c/1463',\n",
    "                       'https://www.obi.de/moebelbau/arbeitsplatten/c/916',\n",
    "                       'https://www.obi.de/moebelbau/regalboeden-moebelbauplatten/c/917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',\n",
    "          'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "          'Accept-Language' : 'nl-NL,nl;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "          'Cache-Control' : 'max-age=0',\n",
    "          'Connection': 'keep-alive',\n",
    "          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36'}\n",
    "\n",
    "s = cfscrape.create_scraper()\n",
    "scraper = cfscrape.create_scraper(sess=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_links = []\n",
    "for n in range(len(products_collection)):\n",
    "    \n",
    "    scraped_html = scraper.get(products_collection[n], headers=header).content\n",
    "    html = etree.HTML(scraped_html)\n",
    "        \n",
    "    # number of pages \n",
    "    path = '//*[@class=\"pagination-bar__link js-queryLink scrollup\"]/span//text()'\n",
    "    pagination = int(html.xpath(path)[-1])\n",
    "    \n",
    "    # how many products should be in the section\n",
    "    path = '//*[@class=\"results-bar__variants hidden-phone hidden-tablet\"]//text()'\n",
    "    articles_qnt = re.findall(r'\\d+(?= Artik)', html.xpath(path)[0])[0]\n",
    "    articles_qnt = int(articles_qnt)\n",
    "    \n",
    "    print('\\nSection:', n+1, '|', '#Pages:', pagination, '|',  '#Products:', articles_qnt, '|', products_collection[n])\n",
    "    \n",
    "    section_links = []\n",
    "    for page in range(1, pagination+1):\n",
    "        \n",
    "        url = products_collection[n] + '?page={}'\n",
    "        url = url.format(page)\n",
    "\n",
    "        scraped_html = scraper.get(url, headers=header).content\n",
    "        html = etree.HTML(scraped_html)\n",
    "\n",
    "        page_links = []\n",
    "        for link in html.xpath('//*[@class=\"product-wrapper wt_ignore\"]//@href'):\n",
    "            if re.findall(r'\\/p\\/\\d\\d\\d+', link):\n",
    "                page_links.append(link)\n",
    "        section_links.extend(page_links)\n",
    "\n",
    "        print('Page: ', page, '|', len(page_links), 'links extracted', '|', 'Total links (mined):', len(section_links), '|', 'Total links (website):', articles_qnt)\n",
    "        if page == pagination and len(section_links) != articles_qnt:\n",
    "            print('Got less links htan expected for section: ', products_collection[n] )\n",
    "            break\n",
    "        elif page == pagination and len(section_links) == articles_qnt:\n",
    "            print('All links were extracted correctly')\n",
    "            pass\n",
    "        \n",
    "        sleep(5)\n",
    "    \n",
    "    all_links.extend(section_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicating links\n",
    "product_links = []\n",
    "for i in all_links:\n",
    "    if i not in product_links:\n",
    "        product_links.append(i)\n",
    "len(product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_links = ['https://www.obi.de' + x for x in product_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OBI-Wood-Links.pkl', 'wb') as f:\n",
    "    pickle.dump(product_links, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_html = scraper.get(product_links[1], headers=header).content\n",
    "html = etree.HTML(scraped_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_options = []\n",
    "plain_pages = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sorting plain and oprional pages\n",
    "for i in range(len(product_links)):\n",
    "    link = product_links[i]\n",
    "    scraped_html = scraper.get(link, headers=header).content\n",
    "    html = etree.HTML(scraped_html)\n",
    "\n",
    "    # Checking multiple options\n",
    "    raw_options = html.xpath('//*[@class=\"selectboxes clearfix marg_t5 marg_b20\"]//text()')\n",
    "    if len(raw_options)>0 and link not in links_options:\n",
    "        links_options.append(link)\n",
    "    elif len(raw_options)==0 and link not in plain_pages:\n",
    "        plain_pages.append(link)\n",
    "    if i%200==0:\n",
    "        print('#', i)\n",
    "    \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links_options), len(plain_pages), len(links_options)+len(plain_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking optiopn patterns\n",
    "opt_df = pd.DataFrame(columns=['Pattern'], index = range(len(links_options)))\n",
    "for i in range(len(links_options)):\n",
    "    link = links_options[i]\n",
    "    scraped_html = scraper.get(link, headers=header).content\n",
    "    html = etree.HTML(scraped_html)\n",
    "    sleep(0.3)\n",
    "    \n",
    "    # Checking multiple options\n",
    "    page_options = []\n",
    "    raw_options = html.xpath('//*[@class=\"selectboxes clearfix marg_t5 marg_b20\"]//text()')\n",
    "    for opt in raw_options:\n",
    "        if ':' in opt:\n",
    "            page_options.append(opt)\n",
    "    opt_df.loc[i] =  ' '.join(page_options)\n",
    "    if i%100==0:\n",
    "        print(i, ' '.join(page_options))\n",
    "    \n",
    "    #sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OBI-links_options.pkl', 'wb') as f:\n",
    "    pickle.dump(links_options, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OBI-opt_df.pkl', 'wb') as f:\n",
    "    pickle.dump(opt_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df['Pattern'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df['Pattern'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df = opt_df.replace('Ã¤', 'ä', regex=True)\n",
    "opt_df = opt_df.replace('Ã¼', 'ü', regex=True)\n",
    "opt_df = opt_df.replace('Ã¶', 'ö', regex=True)\n",
    "opt_df = opt_df.replace('Ã', 'ß', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data inside page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df['Pattern'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=497\n",
    "browser.get(links_options[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting options\n",
    "#### Option 1. One Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df[(opt_df['Pattern'] == 'Farbe:') | \n",
    "       (opt_df['Pattern'] == 'Ausführung:')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "option_single_pages_df = pd.DataFrame([])\n",
    "option_single_pages_df['URL']= np.nan\n",
    "\n",
    "start=0\n",
    "indexes = opt_df[(opt_df['Pattern'] == 'Farbe:') | (opt_df['Pattern'] == 'Ausführung:')].index[start:]\n",
    "\n",
    "for n, i in enumerate(indexes, start=start):\n",
    "    if n%50==0 or n+1 == len(indexes):\n",
    "        sleep(randint(1,2))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Optional product:', n+1, links_options[i])\n",
    "    \n",
    "    if links_options[i] in option_single_pages_df['URL'].unique():\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    scraped_html = scraper.get(links_options[i], headers=header).content\n",
    "    html = etree.HTML(scraped_html)\n",
    "    \n",
    "    # Getting links to all selecting options\n",
    "    opt_links_raw = html.xpath('//*[@class=\"selectboxes clearfix marg_t5 marg_b20\"]//@href') \n",
    "    opt_links = []\n",
    "    for opt_link in opt_links_raw:\n",
    "        if 'https://' in opt_link and opt_link not in opt_links:\n",
    "            opt_links.append(opt_link)\n",
    "    print('Links for product options:', len(opt_links))\n",
    "   \n",
    "    \n",
    "    # Getting data for each option\n",
    "    \n",
    "    option_df = pd.DataFrame([])\n",
    "    for i in range(len(opt_links)):\n",
    "        sleep(0.5)\n",
    "\n",
    "        # Getting page data\n",
    "        url = opt_links[i]\n",
    "        scraped_html = scraper.get(url, headers=header).content\n",
    "        html = etree.HTML(scraped_html)\n",
    "\n",
    "        # Main Table\n",
    "        soup = BeautifulSoup(scraped_html, 'html.parser')\n",
    "        tables = soup.findAll(\"table\", { \"class\" : \"c-datalist c-datalist--33\" })\n",
    "\n",
    "        tables_dict = {}\n",
    "        for count, table in enumerate(tables):\n",
    "            for element in table.find_all('td'):\n",
    "                key = element['data-label']+'-T'+ str(count+1)\n",
    "                tables_dict[key] = element.text\n",
    "        main_table = pd.DataFrame(tables_dict, index=[0])\n",
    "        main_table['URL'] = url\n",
    "\n",
    "        # Title\n",
    "        path = '//*[@class=\"h2 overview__heading\"]//text()'\n",
    "        title = html.xpath(path)\n",
    "        title = '| '.join(title)\n",
    "        main_table['Title'] = title\n",
    "\n",
    "        # SKU\n",
    "        path = '//*[@class=\"article-number\"]//text()'\n",
    "        sku = html.xpath(path)\n",
    "        sku = '| '.join(sku)\n",
    "        main_table['SKU'] = sku\n",
    "\n",
    "        # Description top\n",
    "        path = '//*[@class=\"overview__detail-list normal black\"]//text()'\n",
    "        descr_top = html.xpath(path)\n",
    "        descr_top = '| '.join(descr_top)\n",
    "\n",
    "        # Description bottom\n",
    "        path = '//*[@data-ui-name=\"ads.description-text.product-details.p\"]//text()'\n",
    "        bottom_descr = html.xpath(path)\n",
    "        bottom_descr = '| '.join(bottom_descr)\n",
    "\n",
    "        main_table['Description'] = descr_top + ' | ' + bottom_descr\n",
    "\n",
    "        # Price\n",
    "        path = '//*[@class=\"span12 span-mobile12\"]//text()'\n",
    "        price = html.xpath(path)\n",
    "        price = '| '.join(price)\n",
    "        main_table['Price Raw'] = price\n",
    "\n",
    "        # Delivery\n",
    "        path = '//*[@class=\"button-to-card__txt--table-cell\"]//text()'\n",
    "        delivery = html.xpath(path)\n",
    "        delivery = '| '.join(delivery)\n",
    "        main_table['Delivery'] = delivery\n",
    "\n",
    "        option_df = pd.concat([option_df, main_table], ignore_index=True)\n",
    "        \n",
    "        if ((i+1)%10==0 or i+1==len(all_options)) and i>0:\n",
    "            print('Processed option:', i+1, '|', 'Table length:', option_df.shape[0])\n",
    "            sleep(0.5)\n",
    "        \n",
    "    option_single_pages_df = pd.concat([option_single_pages_df, option_df], ignore_index=True)\n",
    "    \n",
    "    print('Products Dataframe Length:', option_single_pages_df.shape[0])\n",
    "    if option_single_pages_df.shape[0] != option_single_pages_df.drop_duplicates().shape[0]:\n",
    "        print('Products Dataframe Length NO DUPL:', option_single_pages_df.drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_links = [x for x in [links_options[i] for i in indexes] if x not in option_single_pages_df['URL'].unique()]\n",
    "len(missed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_pages.extend(missed_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Two Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#option_pages_df = pd.DataFrame([])\n",
    "#option_pages_df['URL']= np.nan\n",
    "\n",
    "start=0\n",
    "for n, i in enumerate(opt_df[opt_df['Pattern'] == 'Farbe: Ausführung:'].index[start:], start=start):\n",
    "    if n%50==0 or n+1 == opt_df[opt_df['Pattern'] == 'Farbe: Ausführung:'].shape[0]:\n",
    "        sleep(randint(1,2))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Optional product:', n+1, links_options[i])\n",
    "    \n",
    "    if links_options[i] in option_pages_df['URL'].unique():\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    scraped_html = scraper.get(links_options[i], headers=header).content\n",
    "    html = etree.HTML(scraped_html)\n",
    "    \n",
    "    # Getting links to all selecting options\n",
    "    opt_links_raw = html.xpath('//*[@data-ui-name=\"ads.variants.color.enabled\"]//@href') \n",
    "    opt_links = []\n",
    "    for opt_link in opt_links_raw:\n",
    "        if 'https://' in opt_link and opt_link not in opt_links:\n",
    "            opt_links.append(opt_link)\n",
    "    \n",
    "    all_options = []\n",
    "    all_options.extend(opt_links)\n",
    "    for i, opt_link in enumerate(opt_links):\n",
    "        if (i+1)%10==0 or i+1 == len(opt_links):\n",
    "            print(i+1, 'out of', len(opt_links), 'main options', '|', 'Links for product options:', len(all_options))\n",
    "\n",
    "        scraped_html = scraper.get(opt_link, headers=header).content\n",
    "        html = etree.HTML(scraped_html)\n",
    "        child_links = html.xpath('//*[@class=\"selectbox child3\"]//@href')\n",
    "        \n",
    "        sleep(1)\n",
    "        for child_link in child_links:\n",
    "            if child_link not in all_options and 'https://' in child_link:\n",
    "                all_options.append(child_link)\n",
    "    \n",
    "    # Getting data for each option\n",
    "    \n",
    "    option_df = pd.DataFrame([])\n",
    "    for i in range(len(all_options)):\n",
    "        sleep(0.5)\n",
    "\n",
    "        # Getting page data\n",
    "        url = all_options[i]\n",
    "        scraped_html = scraper.get(url, headers=header).content\n",
    "        html = etree.HTML(scraped_html)\n",
    "\n",
    "        # Main Table\n",
    "        soup = BeautifulSoup(scraped_html, 'html.parser')\n",
    "        tables = soup.findAll(\"table\", { \"class\" : \"c-datalist c-datalist--33\" })\n",
    "\n",
    "        tables_dict = {}\n",
    "        for count, table in enumerate(tables):\n",
    "            for element in table.find_all('td'):\n",
    "                key = element['data-label']+'-T'+ str(count+1)\n",
    "                tables_dict[key] = element.text\n",
    "        main_table = pd.DataFrame(tables_dict, index=[0])\n",
    "        main_table['URL'] = url\n",
    "\n",
    "        # Title\n",
    "        path = '//*[@class=\"h2 overview__heading\"]//text()'\n",
    "        title = html.xpath(path)\n",
    "        title = '| '.join(title)\n",
    "        main_table['Title'] = title\n",
    "\n",
    "        # SKU\n",
    "        path = '//*[@class=\"article-number\"]//text()'\n",
    "        sku = html.xpath(path)\n",
    "        sku = '| '.join(sku)\n",
    "        main_table['SKU'] = sku\n",
    "\n",
    "        # Description top\n",
    "        path = '//*[@class=\"overview__detail-list normal black\"]//text()'\n",
    "        descr_top = html.xpath(path)\n",
    "        descr_top = '| '.join(descr_top)\n",
    "\n",
    "        # Description bottom\n",
    "        path = '//*[@data-ui-name=\"ads.description-text.product-details.p\"]//text()'\n",
    "        bottom_descr = html.xpath(path)\n",
    "        bottom_descr = '| '.join(bottom_descr)\n",
    "\n",
    "        main_table['Description'] = descr_top + ' | ' + bottom_descr\n",
    "\n",
    "        # Price\n",
    "        path = '//*[@class=\"span12 span-mobile12\"]//text()'\n",
    "        price = html.xpath(path)\n",
    "        price = '| '.join(price)\n",
    "        main_table['Price Raw'] = price\n",
    "\n",
    "        # Delivery\n",
    "        path = '//*[@class=\"button-to-card__txt--table-cell\"]//text()'\n",
    "        delivery = html.xpath(path)\n",
    "        delivery = '| '.join(delivery)\n",
    "        main_table['Delivery'] = delivery\n",
    "\n",
    "        option_df = pd.concat([option_df, main_table], ignore_index=True)\n",
    "        \n",
    "        if ((i+1)%10==0 or i+1==len(all_options)) and i>0:\n",
    "            print('Processed option:', i+1, '|', 'Table length:', option_df.shape[0])\n",
    "            sleep(0.5)\n",
    "        \n",
    "    option_pages_df = pd.concat([option_pages_df, option_df], ignore_index=True)\n",
    "    \n",
    "    print('Products Dataframe Length:', option_pages_df.shape[0])\n",
    "    if option_pages_df.shape[0] != option_pages_df.drop_duplicates().shape[0]:\n",
    "        print('Products Dataframe Length NO DUPL:', option_pages_df.drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = opt_df[opt_df['Pattern'] == 'Farbe: Ausführung:'].index.to_list()\n",
    "missed_links = [x for x in [links_options[i] for i in idx] if x not in option_pages_df['URL'].unique()]\n",
    "len(missed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_pages.extend(missed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_pages_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_pages_df = option_pages_df.drop_duplicates()\n",
    "option_pages_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data for the pages without selecting options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plain_pages_df = pd.DataFrame([])\n",
    "for i in range(len(plain_pages)):\n",
    "    \n",
    "    if i%100==0:\n",
    "        print('#', i, '|', 'Table length:', plain_pages_df.shape[0])\n",
    "        sleep(randint(1,4))\n",
    "    sleep(1)\n",
    "    \n",
    "    # Getting page data\n",
    "    scraped_html = scraper.get(plain_pages[i], headers=header).content\n",
    "    html = etree.HTML(scraped_html)\n",
    "    \n",
    "    # Main Table\n",
    "    soup = BeautifulSoup(scraped_html, 'html.parser')\n",
    "    tables = soup.findAll(\"table\", { \"class\" : \"c-datalist c-datalist--33\" })\n",
    "    \n",
    "    tables_dict = {}\n",
    "    for i, table in enumerate(tables):\n",
    "        for element in table.find_all('td'):\n",
    "            key = element['data-label']+'-T'+ str(i+1)\n",
    "            tables_dict[key] = element.text\n",
    "    main_table = pd.DataFrame(tables_dict, index=[0])\n",
    "    main_table['URL'] = plain_pages[i]\n",
    "    \n",
    "    # Title\n",
    "    path = '//*[@class=\"h2 overview__heading\"]//text()'\n",
    "    title = html.xpath(path)\n",
    "    title = '| '.join(title)\n",
    "    main_table['Title'] = title\n",
    "    \n",
    "    # SKU\n",
    "    path = '//*[@class=\"article-number\"]//text()'\n",
    "    sku = html.xpath(path)\n",
    "    sku = '| '.join(sku)\n",
    "    main_table['SKU'] = sku\n",
    "    \n",
    "    # Description top\n",
    "    path = '//*[@class=\"overview__detail-list normal black\"]//text()'\n",
    "    descr_top = html.xpath(path)\n",
    "    descr_top = '| '.join(descr_top)\n",
    "    \n",
    "    # Description bottom\n",
    "    path = '//*[@data-ui-name=\"ads.description-text.product-details.p\"]//text()'\n",
    "    bottom_descr = html.xpath(path)\n",
    "    bottom_descr = '| '.join(bottom_descr)\n",
    "    \n",
    "    main_table['Description'] = descr_top + ' | ' + bottom_descr\n",
    "    \n",
    "    # Price\n",
    "    path = '//*[@class=\"span12 span-mobile12\"]//text()'\n",
    "    price = html.xpath(path)\n",
    "    price = '| '.join(price)\n",
    "    main_table['Price Raw'] = price\n",
    "    \n",
    "    # Delivery\n",
    "    path = '//*[@class=\"button-to-card__txt--table-cell\"]//text()' #button-to-card row-fluid disabled\n",
    "    delivery = html.xpath(path)\n",
    "    delivery = '| '.join(delivery)\n",
    "    main_table['Delivery'] = delivery\n",
    "    \n",
    "    plain_pages_df = pd.concat([plain_pages_df, main_table], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_pages_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OBI-Plain-Pages.pkl', 'wb') as f:\n",
    "    pickle.dump(plain_pages_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(plain_pages)):\n",
    "    plain_pages_df['URL'].loc[i] = plain_pages[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.concat([option_single_pages_df, option_pages_df, plain_pages_df], ignore_index=True)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = pd.DataFrame({'Images':np.nan, 'URL':products['URL'].unique()}, index=range(products['URL'].nunique()))\n",
    "images_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "for idx in images_df.index:\n",
    "    try:\n",
    "        url = images_df['URL'].loc[idx]\n",
    "        browser.get(url)\n",
    "        sleep(0.5)\n",
    "\n",
    "        # How many images in gallery\n",
    "        path = '//*[contains(@class, \"ads-previewslider__item slick-slide\")]/a'\n",
    "        els = browser.find_elements_by_xpath(path)\n",
    "\n",
    "        # Click to expand the image\n",
    "        path = '//*[@class=\"ads-slider__zoom-text\"]'\n",
    "        browser.find_elements_by_xpath(path)[0].click()\n",
    "        sleep(0.3)\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    img_urls = []  \n",
    "    if len(els)>1:\n",
    "\n",
    "        for _ in range(len(els)-1):\n",
    "            # Sliding gallery\n",
    "            try:\n",
    "                path = '//*[contains(@class, \"ads-slider__slick slick-initialized slick-slider\")]/button'\n",
    "                browser.find_elements_by_xpath(path)[1].click()\n",
    "                sleep(0.7)\n",
    "\n",
    "                # Get the image\n",
    "                path = '//*[@class=\"pinch-zoom-container\"]/img'\n",
    "                els = browser.find_elements_by_xpath(path)\n",
    "                for el in els:\n",
    "                    img = el.get_attribute('src')\n",
    "                    if img not in img_urls and 'blind.gif' not in img:\n",
    "                        img_urls.append(img)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        try:\n",
    "            # Get the image\n",
    "            path = '//*[@class=\"pinch-zoom-container\"]/img'\n",
    "            els = browser.find_elements_by_xpath(path)\n",
    "            for el in els:\n",
    "                img = el.get_attribute('src')\n",
    "                if img not in img_urls and 'blind.gif' not in img:\n",
    "                    img_urls.append(img)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    print('\\n#', idx, '; '.join(img_urls))\n",
    "    images_df['Images'].loc[idx]= '; '.join(img_urls)\n",
    "\n",
    "    if idx%50==0 and idx>0:\n",
    "        sleep(randint(1,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df['Images'] = images_df['Images'].apply(lambda x: np.nan if x=='' else x)\n",
    "products = pd.merge(products, images_df, on='URL', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products[~products.duplicated(keep='last')]\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.replace('Ã¤', 'ä', regex=True)\n",
    "products = products.replace('Ã¼', 'ü', regex=True)\n",
    "products = products.replace('Ã¶', 'ö', regex=True)\n",
    "products = products.replace('Ã', 'ß', regex=True)\n",
    "products = products.replace('â¬', '€', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OBI-Wood-Products-Raw.pkl', 'wb') as f:\n",
    "    pickle.dump(products, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price/Delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = products.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Price'] = test['Price Raw'].str.replace(r'\\| +\\| +','| ').str.replace(r'^\\D+','')\n",
    "test['Price'] = test['Price'].str.extract(r'(.+?(?=\\|))')[0]\n",
    "test['Price'] = test['Price'].str.replace(r'\\,','\\.').str.replace(r'[^\\.\\d]|\\.00','').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delivery and Order Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Delivery Status'] = test['Delivery'].str.extract(r'(ca\\. \\d+\\-\\d+ .+?(?=\\|))')[0]\n",
    "pat = test['Delivery'].str.extract(r'(sofort lieferbar .+?(?=\\|))')[0]\n",
    "test['Delivery Status'] = test['Delivery Status'].fillna(pat)\n",
    "test['Delivery Yes/No'] = test['Delivery Status'].str.replace(r'.+','Yes').fillna('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SKU'] = test['SKU'].str.replace(r'\\D','')\n",
    "test['SKU'].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop missed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_cols = pd.DataFrame((test.isna().sum()/test.shape[0]*100), columns=['Missed'])\n",
    "cols_del = missed_cols[missed_cols['Missed']>96].index.to_list()\n",
    "len(cols_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=cols_del)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unifying to mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Title-Dims'] = test['Title'].str.replace(r'^.+?(?=\\d)','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.index:\n",
    "    if len(re.findall(r'(?<=\\d) m|(?<=\\d)m|(?<=\\d) cm|(?<=\\d)cm', test['Title-Dims'].loc[i]))==0:\n",
    "        test['Title-Dims'].loc[i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Title-Dims'] = test['Title-Dims'].str.replace(r'[^cmx\\d\\,\\.]','').str.replace(r'^\\D+?(?=\\d)','')\n",
    "test['Title-Dims'] = test['Title-Dims'].str.replace(r'cm[^x]','cm').str.replace(r'mm[^x]','mm')\n",
    "test['Title-Dims'] = test['Title-Dims'].str.replace(r'cm[^x]','cm').str.replace(r'mm[^x]','mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Header-Dims-Mult'] = test['Title-Dims']\n",
    "for i in test.index:\n",
    "    if 'cm' in test['Header-Dims-Mult'].fillna('').loc[i]:\n",
    "        test['Header-Dims-Mult'].loc[i] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.index:\n",
    "    if 'mm' in str(test['Header-Dims-Mult'].fillna('').loc[i]):\n",
    "        test['Header-Dims-Mult'].loc[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dims = test['Title-Dims'].str.split('x', expand=True)\n",
    "header_dims.isna().sum()/header_dims.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dims = header_dims.drop(columns=3).rename(columns={0:'H-Dim1', 1:'H-Dim2', 2:'H-Dim3'})\n",
    "for col in ['H-Dim1', 'H-Dim2', 'H-Dim3']:\n",
    "    header_dims[col] = header_dims[col].str.replace(r'\\,','.').str.replace(r'[^\\.\\d]','')\n",
    "    header_dims[col] = header_dims[col].apply(lambda x: np.nan if x=='' or x ==' '  else x)\n",
    "   # header_dims[col] = header_dims[col].astype(float)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, header_dims], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking missed dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_cols = ['Stärke-T1', 'Breite-T1', 'Länge-T1','Höhe-T2','Breite-T2','Tiefe-T2','H-Dim1','H-Dim2','H-Dim3']\n",
    "test[test['Stärke-T1'].isna()][dims_cols].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Stärke-T1','Breite-T1','Länge-T1','Höhe-T2','Breite-T2','Tiefe-T2']\n",
    "for col in cols:\n",
    "    mult_col = col + '-Mult'\n",
    "    mm_col = col + '-mm'\n",
    "    test[mult_col] = 1\n",
    "    \n",
    "    test[col] = test[col].astype(str)\n",
    "    for i in test.index:\n",
    "        if 'cm' in test[col].loc[i]:\n",
    "            test[mult_col].loc[i] = 10\n",
    "            continue\n",
    "\n",
    "        elif 'mm' in test[col].loc[i]:\n",
    "            test[mult_col].loc[i] = 1\n",
    "            continue\n",
    "\n",
    "        elif 'm' in test[col].loc[i]:\n",
    "            test[mult_col].loc[i] = 1000\n",
    "    test[mult_col] = test[mult_col].astype(float)\n",
    "    test[mm_col] = test[col].str.replace(r'\\,','.').str.replace(r'[^\\.\\d]','')\n",
    "    test[mm_col] = test[mm_col].apply(lambda x: np.nan if x =='' or x==' ' else x).astype(float)\n",
    "    \n",
    "    test[mm_col] = test[mm_col]*test[mult_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dims_cols = ['Stärke-T1','Stärke-T1-mm', \n",
    "                 'Breite-T1', 'Breite-T1-mm',\n",
    "                 'Länge-T1','Länge-T1-mm',\n",
    "                 'Höhe-T2','Höhe-T2-mm',\n",
    "                 'Breite-T2','Breite-T2-mm',\n",
    "                 'Tiefe-T2','Tiefe-T2-mm',\n",
    "                 'H-Dim1','H-Dim2','H-Dim3','Header-Dims-Mult']\n",
    "test[new_dims_cols].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_test = test[new_dims_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Stärke-T1-mm','Breite-T1-mm','Länge-T1-mm']\n",
    "sec_cols = ['Tiefe-T2-mm','Breite-T2-mm','Höhe-T2-mm']\n",
    "for col in cols:\n",
    "    vals=[]\n",
    "    for sec_col in sec_cols: \n",
    "        val = dims_test[(~dims_test[col].isna()) &\n",
    "                          (dims_test[col] - dims_test[sec_col]) == 0].shape[0]\n",
    "        vals.append(val)\n",
    "    print(vals)\n",
    "    print(vals.index(max(vals)))\n",
    "    print(col, sec_cols[vals.index(max(vals))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Stärke-T1-mm'] = test['Stärke-T1-mm'].fillna(test['Tiefe-T2-mm'])\n",
    "test['Länge-T1-mm'] = test['Länge-T1-mm'].fillna(test['Höhe-T2-mm'])\n",
    "test['Breite-T1-mm'] = test['Breite-T1-mm'].fillna(test['Breite-T2-mm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Stärke-T1-mm'].isna()) & \n",
    "     (test['Länge-T1-mm'].isna()) & \n",
    "     (test['Breite-T1-mm'].isna()) &\n",
    "     (~test['H-Dim1'].isna())]['Title'].shape[0]/test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missed Dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Stärke-T1-mm'].isna()) & \n",
    "     (test['Länge-T1-mm'].isna()) & \n",
    "     (test['Breite-T1-mm'].isna())]['Title'].shape[0]/test.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Stärke-T1-mm'].isna()) & \n",
    "     (test['Länge-T1-mm'].isna()) & \n",
    "     (test['Breite-T1-mm'].isna())].shape#['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all columns to mine text form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined'] = ''\n",
    "for col in test.columns[1:]:\n",
    "    test['Combined'] = test['Combined'] + '; ' + test[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined'] = test['Combined'].str.replace(r'\\; nan','').str.replace(r'^; ','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = ['A-Sortierung', 'B-Sortierung', 'C-Sortierung', 'A/B', 'B/C','Kl. A', '1.Wahl Sortierung', 'Altholz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Quality'] = ''\n",
    "for i in test.index:\n",
    "    test['Combined Quality'].loc[i] = []\n",
    "    for qual in quality:\n",
    "        pat = r' {} |^{} | {}$| {}\\;'.format(qual.lower(), qual.lower(), qual.lower(), qual.lower())\n",
    "        if len(re.findall(pat, test['Combined'].loc[i].lower()))>0 and qual.lower() not in ', '.join(test['Combined Quality'].loc[i]):\n",
    "            test['Combined Quality'].loc[i].append(qual)\n",
    "            \n",
    "    try:\n",
    "        test['Combined Quality'].loc[i] = ', '.join(test['Combined Quality'].loc[i])\n",
    "    except:\n",
    "        pass\n",
    "test['Combined Quality'] =  test['Combined Quality'].apply(lambda x: np.nan if x==' ' or x=='' else x )\n",
    "test['Combined Quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_classes = ['S7', 'S10', 'S13', 'C16M', 'C24M', 'C30M', 'D30', 'D35', 'D40',\n",
    "               'D60', 'D70', 'geringe tragfähigkeit', 'mittlere tragfähigkeit',\n",
    "               'hohe tragfähigkeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Sorting Class'] = ''\n",
    "for i in test.index:\n",
    "    test['Combined Sorting Class'].loc[i] = []\n",
    "    for sort_class in sort_classes:\n",
    "        pat = r' {} |^{} | {}$| {}\\;'.format(sort_class.lower(), sort_class.lower(), sort_class.lower(), sort_class.lower())\n",
    "        if len(re.findall(pat, test['Combined'].loc[i].lower()))>0 and sort_class.lower() not in ', '.join(test['Combined Sorting Class'].loc[i]):\n",
    "            test['Combined Sorting Class'].loc[i].append(sort_class)\n",
    "       \n",
    "    try:\n",
    "        test['Combined Sorting Class'].loc[i] = ', '.join(test['Combined Sorting Class'].loc[i])\n",
    "    except:\n",
    "        pass\n",
    "test['Combined Sorting Class'] =  test['Combined Sorting Class'].apply(lambda x: np.nan if x==' ' or x=='' else x )\n",
    "test['Combined Sorting Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wood Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woods = ['Ahorn', 'Tanne', 'Sperrholz', 'Nussbaum', 'Erle',\n",
    "        'Bongossi (Azobé)', 'Birke', 'Buche', 'Douglasie',\n",
    "        'Edelkastanie', 'Eiche', 'Elsbeere', 'Erle', 'Esche', 'Fichte',\n",
    "        'Kiefer', 'Kirschbaum', 'Lärche', 'Linde', 'Mahagoni', 'Teak',\n",
    "        'Pappel', 'Robinie', 'Tanne', 'Ulme', 'Walnuss', 'Weide', 'Birne',\n",
    "        'Fichte/Tanne', 'Fichte/Kiefer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Meta Wood'] = ''\n",
    "for i in test.index:\n",
    "    for wood in woods:\n",
    "        pat = r' {} |^{} | {}$'.format(wood.lower(), wood.lower(), wood.lower())\n",
    "        if len(re.findall(pat, test['Combined'].loc[i].lower()))>0:\n",
    "            test['Meta Wood'].loc[i] = wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Meta Wood'] = test['Meta Wood'].apply(lambda x: np.nan if x==' ' or x=='' else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Wood Type'] = test['Material-T1'].fillna(test['Holzart-T1']).fillna(test['Holzart-Dekor-T1']).fillna(test['Meta Wood'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheking wrong typs and woods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_wood = ['Holzwerkstoff (MDF)', 'Glas', 'Aluminium', 'Metall', 'Granit', 'Polymere', 'Granito',\n",
    "              'Kunststoff','Laminiert','Gipskarton, Furniert','PVC','Beschichtet', 'Melaminbeschichtet' ,\n",
    "              'Furniert','PVC-freie Werkstoffmischung',  'Foliert',  'Laminiert', 'Polymere','Polycarbonat',\n",
    "              'Polyvinylchlorid (PVC)','Gipskarton','German Compact Composite (GCC)','Bandstahl','Foliert',\n",
    "              'Faseprofil','Metall, Kunststoff', 'Textil, Kunststoff','Rotbuche','Multiplex',\n",
    "              'Qualitätsspanplatte mit HPL-Beschichtung Profil aus Hart-PVC','Holz-Kunststoff-Verbundwerkstoffe (WPC)'\n",
    "              ,'Beschichtete Flachpressplatte', 'Hart-PVC-Leiste','Lackiert','WPC', \n",
    "              'Weich-PVC','Unbehandelt','Kunststoffbeschichtet','Kunststoffummantelter Hartfaserkern',\n",
    "               'Stahl','Beschichtete Flachpressplatte, Hart-PVC-Leiste','Beidseitig mit HPL beschichtete Spanplatte',\n",
    "              'Polycarbonat', 'German Compact Composite (GCC)', \n",
    "                'Foliert', 'WPC', 'Multiplex', 'Beschichtet', 'Stahl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of wrong wood types\n",
    "test[(test['Wood Type'].isin(wrong_wood))].shape[0]/test.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Wood Type'].isin(wrong_wood))]['Wood Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[(~test['Wood Type'].isin(wrong_wood))]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Wood Type'] = test['Wood Type'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_surfaces = ['geschliffen', 'gehobelt', 'gefast',\n",
    "                   'sägerau', 'geriffelt', 'glatt gehobelt', 'glatte', 'gefräst',\n",
    "                   'gebürstet', 'strukturiert', '1-Seitig gehobelt',\n",
    "                   '2-Seitig gehobelt', '3-Seitig gehobelt', '4-Seitig gehobelt',\n",
    "                   'genutet', 'unbehandelt', 'naturbelassen', \n",
    "                   'deckend', 'lasiert', 'vorgeölt', 'kesseldruckimprägniert (KDI)',\n",
    "                   'hitzebehandelt', 'thermobehandelt', 'wachsen', 'imprägniert',\n",
    "                   'Scharfkantig']\n",
    "\n",
    "weak_surfaces = ['roh', 'keine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Surface Treat'] = ''\n",
    "for i in test.index:\n",
    "    test['Combined Surface Treat'].loc[i] = []\n",
    "    for surface in weak_surfaces:\n",
    "        pat = r' {} |^{} | {}$'.format(surface.lower(), surface.lower(), surface.lower())\n",
    "        if len(re.findall(pat, test['Combined'].loc[i].lower()))>0 and surface.lower() not in ', '.join(test['Combined Surface Treat'].loc[i]):\n",
    "            test['Combined Surface Treat'].loc[i].append(surface)\n",
    "    \n",
    "    for surface in strong_surfaces:\n",
    "        if surface.lower() in test['Combined'].loc[i].lower() and surface.lower() not in ', '.join(test['Combined Surface Treat'].loc[i]):\n",
    "            test['Combined Surface Treat'].loc[i].append(surface)\n",
    "    try:\n",
    "        test['Combined Surface Treat'].loc[i] = ', '.join(test['Combined Surface Treat'].loc[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Surface Treat'] = test['Combined Surface Treat'].apply(lambda x: np.nan if x==' ' or x=='' else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Surface'] = test['Oberflächenbehandlung-T1'].fillna(test['Oberfläche-T1']).fillna(test['Combined Surface Treat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drying method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_method = ['technischer trocknung','technisch getrocknet', 'kammer getrocknet',\n",
    "               'kammertrocken', 'AD (luftgetrocknet)',\n",
    "               'KD (Künstlich getrocknet)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Drying Method'] = ''\n",
    "for i in test.index:\n",
    "    test['Combined Drying Method'].loc[i] = []\n",
    "    for method in dry_method:\n",
    "        if method.lower() in test['Combined'].loc[i].lower() and method.lower() not in ', '.join(test['Combined Drying Method'].loc[i]):\n",
    "            test['Combined Drying Method'].loc[i].append(method)\n",
    "        \n",
    "    try:\n",
    "        test['Combined Drying Method'].loc[i] = ', '.join(test['Combined Drying Method'].loc[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Drying Method'] =  test['Combined Drying Method'].apply(lambda x: np.nan if x==' ' or x=='' else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Combined Drying Method'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df2gspread import gspread2df as g2d\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "spreadsheet_key = '1iDGux_PxTPSIkxnljeEhMifJo5jAXDhxOXLB8zlrOKg'\n",
    "scope = ['https://spreadsheets.google.com/feeds'] \n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('/Users/macbook/Downloads/gs-credentials.json', scope) \n",
    "keywords = g2d.download(gfile=spreadsheet_key, wks_name = 'Keywords', credentials=credentials, col_names=True, row_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_cols = {'Produktart':'Product Type', \n",
    "               'Holztyp':'Wood Type',\n",
    "               'Qualität':'Quality',\n",
    "               'Oberflächenqualität':'Surface Quality',\n",
    "               'Sortierklasse':'Sorting Class',\n",
    "               'Trocknung':'Drying Method'}\n",
    "keywords = keywords.rename(columns=rename_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in keywords.columns:\n",
    "    keywords[col] = keywords[col].apply(lambda x: np.nan if x == '' or x == ' ' or x== '-777' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords['Wood Type'].loc[31] = 'Holz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Product types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Product Type'] = test['Art-T1'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Product Type New'] = np.nan\n",
    "test['Product Type New'].loc[0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in matched_df.index:\n",
    "    test['Product Type New'].loc[i] = []\n",
    "    for key_word in keywords['Product Type'].unique():\n",
    "        if key_word.lower() in test['Combined'].loc[i].lower():\n",
    "            test['Product Type New'].loc[i].append(key_word)\n",
    "    test['Product Type New'].loc[i] = ', '.join(test['Product Type New'].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Product Type'] = test['Product Type'].fillna(test['Product Type New'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_products = ['Befestigungssysteme', 'Zubehör']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[~test['Product Type'].isin(wrong_products)]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['Product Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ren_cols = {#'Product Type New':'Product Type',\n",
    "           'Combined Sorting Class':'Sorting Class',\n",
    "           'Combined Quality':'Quality',\n",
    "            'Combined Drying Method':'Drying Method',\n",
    "            'Combined Surface Treat':'Surface Treatment',\n",
    "            'Combined Surface Quality':'Surface Quality'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.rename(columns=ren_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cols = ['URL','SKU','Product Type','Wood Type','Surface Treatment',\n",
    "             'Drying Method','Sorting Class','Quality','Combined'\n",
    "            #'Header','Surface Quality',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Matched Keywords'] = np.nan\n",
    "test['Other Keywords'] = np.nan\n",
    "test = test.fillna('-777')\n",
    "keywords = keywords.fillna('-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_keywords(row):\n",
    "    row['Matched Keywords'] = {}\n",
    "    row['Other Keywords'] = {}\n",
    "    \n",
    "    matched_cols=[]\n",
    "    for col in ['Product Type','Wood Type','Surface Treatment',#'Surface Quality',\n",
    "                'Drying Method','Sorting Class','Quality']:\n",
    "        \n",
    "        for key_word in keywords[col].unique():\n",
    "            if fuzz.token_set_ratio(str(row[col]).lower(), key_word.lower()) > 90 or key_word.lower() in str(row[col]):\n",
    "                row['Matched Keywords'][row[col]] = key_word\n",
    "                matched_cols.append(col)\n",
    "    \n",
    "    if len(matched_cols)==0:\n",
    "        row['Other Keywords'] = np.nan\n",
    "        row['Matched Keywords'] = np.nan\n",
    "    \n",
    "    for col in ['Product Type','Wood Type','Surface Treatment',#'Surface Quality',\n",
    "                'Drying Method','Sorting Class','Quality']:\n",
    "        \n",
    "        if col not in matched_cols and len(matched_cols)>0 and row[col]!=-999:\n",
    "            row['Other Keywords'][col] = row[col]\n",
    "    \n",
    "    return row\n",
    "test = test.apply(get_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Matched Keywords','Other Keywords']].isna().sum()/test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning NaNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    test[col] =  test[col].apply(lambda x: np.nan if x =='' or x == 'nan' or x == ' ' or x == '-777' or x == '-999' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Matched Keywords'].isna())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Matched Keywords'].isna()) & \n",
    "          (test['Wood Type']=='Holz')][main_cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(~test['Matched Keywords'].isna()) #&\n",
    "          ]['Product Type'].nunique()#value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    test[col] = test[col].apply(lambda x: np.nan if x == '' or x == ' ' or x=='-999' or x=='-777' or x=='nan'else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from df2gspread import df2gspread as d2g\n",
    "scope = ['https://spreadsheets.google.com/feeds'] \n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('/Users/macbook/Downloads/gs-credentials.json', scope) \n",
    "gc = gspread.authorize(credentials)\n",
    "spreadsheet_key = '1iDGux_PxTPSIkxnljeEhMifJo5jAXDhxOXLB8zlrOKg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_clean = test[~test['Matched Keywords'].isna()]\n",
    "sheets_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_clean = sheets_clean[(~sheets_clean.duplicated(subset='URL', keep='last'))]\n",
    "sheets_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_clean['Website Name'] = 'www.obi.de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ren_cols = {'Title':'Product Title',\n",
    "            'Breite-T1-mm':'Width (mm)',\n",
    "            'Länge-T1-mm':'Length (mm)', \n",
    "            'Stärke-T1-mm':'Thickness (mm)',\n",
    "            'Price':'Price (Euro)'}\n",
    "sheets_clean = sheets_clean.rename(columns = ren_cols)\n",
    "\n",
    "cols = ['Website Name', 'URL','Product Title','Product Type', 'Wood Type', 'Width (mm)', 'Length (mm)', 'Thickness (mm)', \n",
    "        'Price (Euro)','Delivery Yes/No','Quality', 'Surface Treatment','Drying Method', #'Surface Quality',\n",
    "        'Sorting Class','SKU','Delivery Status' ]\n",
    "\n",
    "sheets_clean = sheets_clean[cols].rename(columns = ren_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_clean = sheets_clean[~sheets_clean['Product Type'].isin(wrong_products)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_clean[(sheets_clean['Width (mm)'].isna()) &\n",
    "             (sheets_clean['Length (mm)'].isna()) &\n",
    "             (sheets_clean['Thickness (mm)'].isna())].shape[0]/sheets_clean.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2g.upload(sheets_clean.fillna('-'),\n",
    "           spreadsheet_key,\n",
    "           'OBI Clean Data',\n",
    "           credentials=credentials,\n",
    "           col_names=True,\n",
    "           row_names=False,\n",
    "           start_cell = 'A1',\n",
    "           clean=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
